"""
Data Extractor Module

Handles web crawling and data extraction from job listings and blog posts.
This module focuses on the extraction process, using schemas generated by schema_generator.py.

Usage:
  python data_extractor.py <jobs_listing_url> <blogs_listing_url>

Notes:
- Each positional argument is optional and defaults to an empty string if omitted.
- Uses schemas from schema_generator.py for extraction rules.
"""

from __future__ import annotations

from dataclasses import dataclass
from pathlib import Path
import json
import os
import re
import hashlib
from typing import Any, Dict, List, Optional, Tuple
from urllib.parse import urlparse, urljoin, urlunparse
import time
import asyncio
import random
from html.parser import HTMLParser
from urllib import request as urlrequest
from urllib import error as urlerror

# Import schema management functions
from .schema_generator import (
    ensure_schema,
    load_latest_schema,
    normalize_domain,
    get_required_fields,
    _llm_available,
    _utc_now
)




RUNTIME_DIRS = ("schemas", "state", "cache", "output")


def ensure_runtime_directories(base_dir: Path | str) -> None:
	"""Create required runtime directories if they do not exist."""
	base_path = Path(base_dir)
	for dirname in RUNTIME_DIRS:
		(base_path / dirname).mkdir(parents=True, exist_ok=True)


def load_env_vars_from_dotenv() -> None:
	"""Load environment variables from a .env file if python-dotenv is installed."""
	try:
		from dotenv import load_dotenv  # type: ignore
		# Load from default .env in current working directory, do not override existing env vars
		load_dotenv()
	except Exception:
		# Silently ignore if python-dotenv is not installed
		pass


# Crawl4AI configuration (polite defaults)
DEFAULT_USER_AGENT = (
	"data-extractor/1.0 (+https://github.com/; contact=ops@example.com)"
)


@dataclass(frozen=True)
class CrawlerSettings:
	user_agent: str = DEFAULT_USER_AGENT
	respect_robots: bool = True
	max_concurrency: int = 2
	delay_seconds: float = 0.2
	cache_dir: Path = Path("cache")


def build_crawler_settings(base_dir: Path) -> CrawlerSettings:
	"""Return polite default Crawl4AI settings anchored to the runtime cache dir."""
	return CrawlerSettings(cache_dir=base_dir / "cache")


def get_crawl_client(settings: CrawlerSettings):
	"""Lazily create a Crawl4AI client using provided settings.

	This defers imports so the script can run even if Crawl4AI is not installed yet.
	Actual crawling will be wired in subsequent steps.
	"""
	try:
		# Delayed import to avoid hard dependency during early steps
		import crawl4ai  # type: ignore

		# Placeholder: return a lightweight handle with settings.
		# The concrete client/session will be constructed when we start fetching.
		return {
			"engine": "crawl4ai",
			"settings": settings,
		}
	except Exception as exc:  # pragma: no cover - informational only
		# We won't use the client until Crawl4AI is installed; keep a stub.
		return {
			"engine": "unavailable",
			"settings": settings,
			"error": str(exc),
		}


# ===== Listing discovery (anchors parsing, pagination, dedupe) =====


class AnchorLinkCollector(HTMLParser):
	"""Collect <a> tags with href, text, rel, class, id."""

	def __init__(self) -> None:
		super().__init__()
		self._in_a = False
		self._current: Dict[str, Any] = {}
		self.anchors: List[Dict[str, Any]] = []

	def handle_starttag(self, tag: str, attrs: List[tuple[str, Optional[str]]]) -> None:
		if tag.lower() == "a":
			self._in_a = True
			attr_map = {k.lower(): (v or "") for k, v in attrs}
			self._current = {
				"href": attr_map.get("href", ""),
				"rel": attr_map.get("rel", ""),
				"class": attr_map.get("class", ""),
				"id": attr_map.get("id", ""),
				"text": "",
			}

	def handle_data(self, data: str) -> None:
		if self._in_a and self._current is not None:
			self._current["text"] = (self._current.get("text", "") + data)

	def handle_endtag(self, tag: str) -> None:
		if tag.lower() == "a" and self._in_a and self._current:
			self._current["text"] = (self._current.get("text", "") or "").strip()
			self.anchors.append(self._current)
			self._current = {}
			self._in_a = False


def parse_anchors(html: str) -> List[Dict[str, Any]]:
	parser = AnchorLinkCollector()
	try:
		parser.feed(html)
	except Exception:
		pass
	return parser.anchors


def canonicalize_url(base_url: str, href: str) -> Optional[str]:
	if not href:
		return None
	try:
		abs_url = urljoin(base_url, href)
		parsed = urlparse(abs_url)
		# Drop fragments
		clean = parsed._replace(fragment="")
		return urlunparse(clean)
	except Exception:
		return None


def same_registered_domain(url_a: str, url_b: str) -> bool:
	def _host(u: str) -> str:
		p = urlparse(u)
		h = (p.netloc or p.path).lower()
		return h[4:] if h.startswith("www.") else h
	return _host(url_a) == _host(url_b)


def _looks_like_nav_or_utility(anchor: Dict[str, Any]) -> bool:
	classes = (anchor.get("class") or "").lower()
	ident = (anchor.get("id") or "").lower()
	text = (anchor.get("text") or "").lower()
	combined = " ".join([classes, ident, text])
	for token in (
		"nav",
		"menu",
		"footer",
		"logo",
		"social",
		"login",
		"signup",
		"register",
		"language",
		"search",
		"contact",
		"support",
	):
		if token in combined:
			return True
	return False


def _is_candidate_item_link(href: str, scrape_type: str) -> bool:
	path = urlparse(href).path.lower()
	host = _extract_host(href)
	# If it's an ATS job page, treat as candidate early
	if scrape_type == "jobs" and is_allowed_ats_url(href):
		return True
	if scrape_type == "blogs":
		# Common blog patterns
		if "/blog/" in path and not path.rstrip("/").endswith("/blog"):
			return True
		if "/blogs/" in path and not path.rstrip("/").endswith("/blogs"):
			return True
		if "/article" in path or "/post/" in path or "/posts/" in path or path.endswith(".html"):
			return True
		# Other common content paths
		for k in ("/insight/", "/insights/", "/news/", "/story/", "/stories/"):
			if k in path:
				# avoid listing roots like /news or /stories
				if not path.rstrip("/").endswith(k.rstrip("/")):
					return True
		# Year/month permalinks (e.g., /2025/08/title)
		try:
			import re as _re  # local alias to avoid top-level changes
			if _re.search(r"/20\d{2}/(0[1-9]|1[0-2])(/|$)", path):
				return True
		except Exception:
			pass
	elif scrape_type == "jobs":
		# Common careers patterns
		keywords = (
			"/careers/",
			"/career/",
			"/jobs/",
			"/job/",
			"/open-positions",
			"/positions/",
			"/opportunities/",
			"/apply/",
		)
		for k in keywords:
			if k in path and not path.rstrip("/").endswith(k.rstrip("/")):
				return True
		# Fallback: company-specific hosts e.g., applytojob.com
		if host.endswith("applytojob.com") or host.startswith("jobs.") or host.startswith("careers."):
			return True
	return False


def _extract_host(url: str) -> str:
	p = urlparse(url)
	h = (p.netloc or p.path).lower()
	return h[4:] if h.startswith("www.") else h


# Common external ATS/job board domains to allow for jobs discovery
ATS_DOMAIN_SUFFIXES: tuple[str, ...] = (
	"greenhouse.io",
	"boards.greenhouse.io",
	"lever.co",
	"jobs.lever.co",
	"myworkdayjobs.com",
	"workday.com",
	"smartrecruiters.com",
	"icims.com",
	"ashbyhq.com",
	"workable.com",
	"bamboohr.com",
	"recruitee.com",
	"oraclecloud.com",
	"taleo.net",
	"jobvite.com",
	"eightfold.ai",
	"paylocity.com",
)


def is_allowed_ats_url(url: str) -> bool:
	host = _extract_host(url)
	for suffix in ATS_DOMAIN_SUFFIXES:
		if host == suffix or host.endswith("." + suffix):
			return True
	return False


def is_greenhouse_url(url: str) -> bool:
	host = _extract_host(url)
	return host.endswith("greenhouse.io") or host.endswith("boards.greenhouse.io") or host.endswith("job-boards.greenhouse.io")


def _find_next_page_url(base_url: str, anchors: List[Dict[str, Any]]) -> Optional[str]:
	# Priority 1: rel="next"
	for a in anchors:
		rel = (a.get("rel") or "").lower()
		if "next" in rel:
			h = canonicalize_url(base_url, a.get("href") or "")
			if h:
				return h
	# Priority 2: text contains next/older
	for a in anchors:
		text = (a.get("text") or "").strip().lower()
		if text in {"next", "next ›", "older", ">", "→", "load more", "more"} or "next" in text:
			h = canonicalize_url(base_url, a.get("href") or "")
			if h:
				return h
	# Priority 3: page param increment heuristics
	for a in anchors:
		href = a.get("href") or ""
		if "page=" in href or "p=" in href:
			h = canonicalize_url(base_url, href)
			if h:
				return h
	return None


def polite_sleep(seconds: float) -> None:
	# add small jitter
	jitter = min(0.5, seconds * 0.2)
	to_sleep = max(0.0, seconds + random.uniform(-jitter, jitter))
	time.sleep(to_sleep)


def _sha1_hex(text: str) -> str:
	return hashlib.sha1(text.encode("utf-8")).hexdigest()


def _fetch_html_with_crawl4ai(url: str) -> Optional[str]:
	"""Try to fetch HTML using Crawl4AI AsyncWebCrawler. Returns HTML or None."""
	try:
		from crawl4ai import AsyncWebCrawler  # type: ignore

		async def _run() -> Optional[str]:
			try:
				async with AsyncWebCrawler(verbose=False) as crawler:
					res = await crawler.arun(url=url)
					# Prefer raw HTML-like fields; fallback to markdown wrapped in HTML
					for attr in ("html", "cleaned_html", "content", "raw_html"):
						html = getattr(res, attr, None)
						if html:
							return html
					markdown = getattr(res, "markdown", None)
					if markdown:
						return f"<html><body><pre>{markdown}</pre></body></html>"
					return None
			except Exception:
				return None

		return asyncio.run(_run())
	except Exception:
		return None


def fetch_html(url: str, settings: CrawlerSettings, use_cache: bool = True) -> Optional[str]:
	# On-disk cache
	cache_dir = Path(settings.cache_dir) / "http"
	if use_cache:
		cache_dir.mkdir(parents=True, exist_ok=True)
		cache_path = cache_dir / f"{_sha1_hex(url)}.html"
		if cache_path.exists():
			try:
				return cache_path.read_text(encoding="utf-8", errors="replace")
			except Exception:
				pass

	# Try Crawl4AI first
	html = _fetch_html_with_crawl4ai(url)
	if not html:
		# Fallback lightweight fetch using urllib
		req = urlrequest.Request(url, headers={"User-Agent": settings.user_agent})
		try:
			with urlrequest.urlopen(req, timeout=30) as resp:  # nosec - controlled URL
				content_type = resp.headers.get("Content-Type", "").lower()
				charset = "utf-8"
				match = re.search(r"charset=([\w-]+)", content_type)
				if match:
					charset = match.group(1)
				data = resp.read()
				try:
					html = data.decode(charset, errors="replace")
				except Exception:
					html = data.decode("utf-8", errors="replace")
		except urlerror.URLError:
			html = None

	# Write into cache if we have html
	if use_cache and html:
		try:
			cache_path.write_text(html, encoding="utf-8", errors="replace")
		except Exception:
			pass
	return html


def discover_listing_items(
	listing_url: str,
	scrape_type: str,
	settings: CrawlerSettings,
	max_pages: int = 2,
) -> Dict[str, Any]:
	"""Return dict with discovered item URLs and stats.

	{"items": [urls...], "pages_visited": N}
	"""
	base = listing_url
	visited_pages = 0
	seen_items: set[str] = set()
	current_url: Optional[str] = base

	while current_url and visited_pages < max_pages:
		polite_sleep(settings.delay_seconds)
		html = fetch_html(current_url, settings)
		visited_pages += 1
		if not html:
			break
		anchors = parse_anchors(html)
		# Gather candidate absolute URLs
		candidates: List[str] = []
		for a in anchors:
			href = a.get("href") or ""
			abs_url = canonicalize_url(current_url, href)
			if not abs_url:
				continue
			# Domain filter: allow all external links for both jobs and blogs
			if _looks_like_nav_or_utility(a):
				continue
			candidates.append(abs_url)
		# Filter to item-looking links
		new_items = [u for u in candidates if _is_candidate_item_link(u, scrape_type)]
		before = len(seen_items)
		seen_items.update(new_items)
		added = len(seen_items) - before
		# Decide pagination
		next_url = _find_next_page_url(current_url, anchors)
		# Only follow if we gained new items or we haven't tried any next yet
		if next_url and (added > 0 or visited_pages == 1):
			current_url = next_url
			continue
		break

	return {"items": sorted(seen_items), "pages_visited": visited_pages}


def write_discovered_urls(base_dir: Path, scrape_type: str, urls: List[str]) -> Path:
	filename = f"discovered_{scrape_type}_urls.txt"
	path = base_dir / "state" / filename
	with path.open("w", encoding="utf-8") as f:
		for u in urls:
			f.write(u + "\n")
	return path


# ===== Data Extraction Functions =====


# ===== Item extraction (JSON-LD, meta tags, regex rules) =====


def strip_html_tags(value: str) -> str:
	return re.sub(r"<[^>]+>", " ", value)


def normalize_whitespace(value: str) -> str:
	return re.sub(r"\s+", " ", value).strip()


def find_json_ld_blocks(html: str) -> List[str]:
	# Prefer BeautifulSoup for robust parsing, fallback to regex
	blocks: List[str] = []
	try:
		from bs4 import BeautifulSoup  # type: ignore
		soup = BeautifulSoup(html, "html.parser")
		for script in soup.find_all("script"):
			type_attr = (script.get("type") or "").lower()
			if "ld+json" in type_attr:
				text = script.string or script.get_text() or ""
				if text.strip():
					blocks.append(text)
	except Exception:
		pass
	# Fallback regex (handles double-quoted type attr)
	if not blocks:
		for m in re.finditer(r"<script[^>]*type=[\"\']application/ld\+json[\"\'][^>]*>([\s\S]*?)</script>", html, re.IGNORECASE):
			blocks.append(m.group(1))
	return blocks


def parse_json_ld_objects(html: str) -> List[Dict[str, Any]]:
	objs: List[Dict[str, Any]] = []
	for block in find_json_ld_blocks(html):
		# Try direct parse; if fail, try to clean up common issues
		try:
			data = json.loads(block)
		except Exception:
			try:
				clean = block.strip().replace("\n", " ")
				data = json.loads(clean)
			except Exception:
				continue
		if isinstance(data, list):
			for item in data:
				if isinstance(item, dict):
					objs.append(item)
		elif isinstance(data, dict):
			objs.append(data)
	return objs


def _match_type(obj: Dict[str, Any], types: Tuple[str, ...]) -> bool:
	val = obj.get("@type")
	if isinstance(val, list):
		return any(str(v).lower() in {t.lower() for t in types} for v in val)
	if isinstance(val, str):
		return val.lower() in {t.lower() for t in types}
	return False


def _extract_text(value: Any) -> Optional[str]:
	if value is None:
		return None
	if isinstance(value, str):
		return normalize_whitespace(strip_html_tags(value))
	return normalize_whitespace(strip_html_tags(json.dumps(value, ensure_ascii=False)))


def _extract_author_name(author: Any) -> Optional[str]:
	if isinstance(author, list):
		names = []
		for a in author:
			if isinstance(a, dict) and a.get("name"):
				names.append(str(a["name"]))
			elif isinstance(a, str):
				names.append(a)
		return ", ".join(names) if names else None
	if isinstance(author, dict) and author.get("name"):
		return str(author["name"])
	if isinstance(author, str):
		return author
	return None


def _extract_image_url(image: Any) -> Optional[str]:
	if isinstance(image, str):
		return image
	if isinstance(image, dict) and image.get("url"):
		return str(image["url"])
	return None


def extract_from_json_ld_jobs(objs: List[Dict[str, Any]], page_url: str) -> Dict[str, Any]:
	for obj in objs:
		# handle @graph nesting
		candidates: List[Dict[str, Any]] = []
		if "@graph" in obj and isinstance(obj["@graph"], list):
			candidates.extend([x for x in obj["@graph"] if isinstance(x, dict)])
		else:
			candidates.append(obj)
		for c in candidates:
			if _match_type(c, ("JobPosting",)):
				record: Dict[str, Any] = {}
				record["title"] = _extract_text(c.get("title"))
				hiring_org = c.get("hiringOrganization") or {}
				if isinstance(hiring_org, dict):
					record["company"] = _extract_text(hiring_org.get("name"))
				else:
					record["company"] = _extract_text(hiring_org)
				job_location = c.get("jobLocation")
				location_text: Optional[str] = None
				if isinstance(job_location, dict):
					addr = job_location.get("address") or {}
					if isinstance(addr, dict):
						parts = [addr.get("addressLocality"), addr.get("addressRegion"), addr.get("addressCountry")]
						location_text = normalize_whitespace(" ".join([p for p in parts if p]))
				elif isinstance(job_location, list) and job_location:
					first = job_location[0]
					if isinstance(first, dict):
						addr = first.get("address") or {}
						if isinstance(addr, dict):
							parts = [addr.get("addressLocality"), addr.get("addressRegion"), addr.get("addressCountry")]
							location_text = normalize_whitespace(" ".join([p for p in parts if p]))
				record["location"] = location_text
				record["description"] = _extract_text(c.get("description"))
				record["posting_date"] = _extract_text(c.get("datePosted"))
				apply_url = c.get("url") or c.get("applicationUrl") or c.get("hiringOrganization", {}).get("url") if isinstance(c.get("hiringOrganization"), dict) else None
				record["apply_url"] = str(apply_url) if apply_url else None
				# Optional fields
				emp_type = c.get("employmentType")
				record["employment_type"] = _extract_text(emp_type)
				base_salary = c.get("baseSalary")
				if isinstance(base_salary, dict):
					val = base_salary.get("value")
					if isinstance(val, dict):
						amount = val.get("value") or val.get("minValue")
						currency = base_salary.get("currency") or val.get("currency")
						record["salary"] = normalize_whitespace(f"{amount} {currency}" if amount else str(val))
					else:
						record["salary"] = _extract_text(val)
				else:
					record["salary"] = _extract_text(base_salary)
				return record
	return {}


def extract_from_json_ld_blogs(objs: List[Dict[str, Any]], page_url: str) -> Dict[str, Any]:
	for obj in objs:
		candidates: List[Dict[str, Any]] = []
		if "@graph" in obj and isinstance(obj["@graph"], list):
			candidates.extend([x for x in obj["@graph"] if isinstance(x, dict)])
		else:
			candidates.append(obj)
		for c in candidates:
			if _match_type(c, ("BlogPosting", "Article", "NewsArticle")):
				record: Dict[str, Any] = {}
				record["title"] = _extract_text(c.get("headline") or c.get("name"))
				record["author"] = _extract_author_name(c.get("author"))
				record["published_date"] = _extract_text(c.get("datePublished"))
				keywords = c.get("keywords")
				if isinstance(keywords, list):
					record["tags"] = [str(k) for k in keywords]
				elif isinstance(keywords, str):
					record["tags"] = [t.strip() for t in re.split(r",|;|\|", keywords) if t.strip()]
				else:
					record["tags"] = []
				body = c.get("articleBody") or c.get("description")
				record["content"] = _extract_text(body)
				record["hero_image"] = _extract_image_url(c.get("image"))
				return record
	return {}


def parse_meta_tags(html: str) -> List[Dict[str, str]]:
	tags: List[Dict[str, str]] = []
	for m in re.finditer(r"<meta\s+([^>]+)/?>", html, re.IGNORECASE):
		attrs = m.group(1)
		name_m = re.search(r"\bname=\"([^\"]+)\"", attrs, re.IGNORECASE)
		prop_m = re.search(r"\bproperty=\"([^\"]+)\"", attrs, re.IGNORECASE)
		content_m = re.search(r"\bcontent=\"([^\"]*)\"", attrs, re.IGNORECASE)
		if content_m and (name_m or prop_m):
			entry = {
				"name": (name_m.group(1).lower() if name_m else ""),
				"property": (prop_m.group(1).lower() if prop_m else ""),
				"content": content_m.group(1),
			}
			tags.append(entry)
	return tags


def extract_via_meta(html: str, scrape_type: str, page_url: str) -> Dict[str, Any]:
	meta = parse_meta_tags(html)
	by_name = {m["name"]: m["content"] for m in meta if m["name"]}
	by_prop = {m["property"]: m["content"] for m in meta if m["property"]}
	if scrape_type == "blogs":
		rec: Dict[str, Any] = {}
		rec["title"] = by_prop.get("og:title") or by_name.get("twitter:title")
		rec["author"] = by_name.get("author")
		rec["published_date"] = by_prop.get("article:published_time") or by_name.get("date")
		# tags via repeated article:tag or keywords
		tags: List[str] = []
		for m in meta:
			if m["property"] == "article:tag" and m["content"]:
				tags.append(m["content"]) 
		if not tags and by_name.get("keywords"):
			tags = [t.strip() for t in by_name["keywords"].split(",") if t.strip()]
		rec["tags"] = tags
		rec["content"] = by_prop.get("og:description") or by_name.get("description")
		return {k: (normalize_whitespace(v) if isinstance(v, str) else v) for k, v in rec.items() if v}
	elif scrape_type == "jobs":
		rec = {}
		rec["title"] = by_prop.get("og:title") or by_name.get("twitter:title")
		rec["description"] = by_prop.get("og:description") or by_name.get("description")
		# posting_date often not in meta; leave empty if unavailable
		return {k: (normalize_whitespace(v) if isinstance(v, str) else v) for k, v in rec.items() if v}
	return {}


def extract_apply_link(html: str, page_url: str) -> Optional[str]:
	anchors = parse_anchors(html)
	for a in anchors:
		text = (a.get("text") or "").strip().lower()
		href = a.get("href") or ""
		if "apply" in text or ("apply" in href.lower()):
			absu = canonicalize_url(page_url, href)
			if absu:
				return absu
	# Look for form actions
	try:
		from bs4 import BeautifulSoup  # type: ignore
		soup = BeautifulSoup(html, "html.parser")
		form = soup.select_one("form[action], form#application_form")
		if form:
			action = form.get("action")
			if action:
				absu = canonicalize_url(page_url, action)
				if absu:
					return absu
	except Exception:
		pass
	return None


def apply_schema_rules(html: str, page_url: str, scrape_type: str, schema: Dict[str, Any]) -> Dict[str, Any]:
	# Support kinds: meta, regex, css, xpath
	rules = schema.get("extraction_rules") or {}
	if not isinstance(rules, dict):
		return {}
	result: Dict[str, Any] = {}
	# Prepare parsers
	soup = None
	xdoc = None
	try:
		from bs4 import BeautifulSoup  # type: ignore
		soup = BeautifulSoup(html, "html.parser")
	except Exception:
		soup = None
	try:
		from lxml import html as lxml_html  # type: ignore
		xdoc = lxml_html.fromstring(html)
	except Exception:
		xdoc = None

	for field, attempts in rules.items():
		if not isinstance(attempts, list):
			continue
		for attempt in attempts:
			kind = (attempt.get("kind") or "").lower()
			if kind == "meta":
				name = (attempt.get("name") or "").lower()
				prop = (attempt.get("property") or "").lower()
				content = None
				meta = parse_meta_tags(html)
				for m in meta:
					if name and m.get("name") == name:
						content = m.get("content")
						break
					if prop and m.get("property") == prop:
						content = m.get("content")
						break
				if content:
					result[field] = normalize_whitespace(content)
					break
			elif kind == "regex":
				pattern = attempt.get("pattern")
				group = int(attempt.get("group", 0))
				if pattern:
					m = re.search(pattern, html, re.IGNORECASE | re.DOTALL)
					if m:
						try:
							val = m.group(group)
							result[field] = normalize_whitespace(strip_html_tags(val))
							break
						except Exception:
							pass
			elif kind == "css" and soup is not None:
				selector = attempt.get("value") or attempt.get("selector") or ""
				attr = attempt.get("attr") or attempt.get("attribute")
				many = bool(attempt.get("many"))
				if selector:
					els = soup.select(selector)
					if els:
						if many:
							vals: List[str] = []
							for el in els:
								if attr:
									val = el.get(attr)
								else:
									val = el.get_text(" ", strip=True)
								if val:
									vals.append(normalize_whitespace(val))
							if vals:
								result[field] = vals
								break
						else:
							el = els[0]
							val = el.get(attr) if attr else el.get_text(" ", strip=True)
							if val:
								result[field] = normalize_whitespace(val)
								break
			elif kind == "xpath" and xdoc is not None:
				expr = attempt.get("value") or attempt.get("selector") or ""
				attr = attempt.get("attr") or attempt.get("attribute")
				many = bool(attempt.get("many"))
				if expr:
					try:
						nodes = xdoc.xpath(expr)
						if nodes:
							if many:
								vals: List[str] = []
								for n in nodes:
									text = None
									if isinstance(n, str):
										text = n
									else:
										if attr:
											text = n.get(attr)
										else:
											text = n.text_content()
									if text:
										vals.append(normalize_whitespace(text))
								if vals:
									result[field] = vals
									break
						else:
							n = nodes[0]
							if isinstance(n, str):
								val = n
							else:
								val = n.get(attr) if attr else n.text_content()
							if val:
								result[field] = normalize_whitespace(val)
								break
					except Exception:
						pass
	return result


def build_record(
	page_url: str,
	scrape_type: str,
	schema: Dict[str, Any],
	jsonld_rec: Dict[str, Any],
	meta_rec: Dict[str, Any],
	rule_rec: Dict[str, Any],
) -> Dict[str, Any]:
	rec: Dict[str, Any] = {}
	# Merge precedence: rules > jsonld > meta for core fields
	if scrape_type == "jobs":
		fields = get_required_fields("jobs") + ["salary", "employment_type"]
	else:
		fields = get_required_fields("blogs") + ["hero_image", "author_profile_url"]
	for f in fields:
		if f in rule_rec and rule_rec[f]:
			rec[f] = rule_rec[f]
		elif f in jsonld_rec and jsonld_rec[f]:
			rec[f] = jsonld_rec[f]
		elif f in meta_rec and meta_rec[f]:
			rec[f] = meta_rec[f]
	# Ensure tags is a list for blogs
	if scrape_type == "blogs":
		if isinstance(rec.get("tags"), str):
			rec["tags"] = [rec["tags"]]
		elif rec.get("tags") is None:
			rec["tags"] = []
	# Defaults
	rec["source_url"] = page_url
	rec["schema_version"] = int(schema.get("version", 1))
	# Greenhouse company fallback from URL path segment if missing
	if scrape_type == "jobs" and (not rec.get("company")) and is_greenhouse_url(page_url):
		p = urlparse(page_url)
		parts = [seg for seg in (p.path or "").split("/") if seg]
		if parts:
			company_slug = parts[0]
			rec["company"] = company_slug.replace("-", " ").title()
	# Apply URL fallback to page URL if still missing (onsite form)
	if scrape_type == "jobs" and not rec.get("apply_url"):
		rec["apply_url"] = page_url
	return {k: v for k, v in rec.items() if v not in (None, "")}


def validate_required(rec: Dict[str, Any], scrape_type: str) -> List[str]:
	required = get_required_fields(scrape_type)
	# Treat some fields as optional if unavailable but non-critical
	if scrape_type == "jobs":
		soft_optional = {"posting_date"}
		required = [f for f in required if f not in soft_optional]
	elif scrape_type == "blogs":
		soft_optional = {"author", "published_date"}
		required = [f for f in required if f not in soft_optional]
	missing = []
	for f in required:
		if f not in rec or rec[f] in (None, ""):
			missing.append(f)
	return missing


def extract_single_item(
	url: str,
	scrape_type: str,
	schema: Dict[str, Any],
	settings: CrawlerSettings,
) -> Tuple[Optional[Dict[str, Any]], Optional[Dict[str, Any]]]:
	polite_sleep(settings.delay_seconds)
	# For Greenhouse, prefer urllib fallback (some responses via Crawl4AI may not include raw HTML)
	if is_greenhouse_url(url):
		html = fetch_html(url, settings, use_cache=True)
	else:
		html = fetch_html(url, settings)
	if not html:
		return None, None
	objs = parse_json_ld_objects(html)
	if scrape_type == "jobs":
		jsonld_rec = extract_from_json_ld_jobs(objs, url)
		meta_rec = extract_via_meta(html, "jobs", url)
		rule_rec = apply_schema_rules(html, url, "jobs", schema)
		core = build_record(url, "jobs", schema, jsonld_rec, meta_rec, rule_rec)
		# supplement apply_url
		if not core.get("apply_url"):
			al = extract_apply_link(html, url)
			if al:
				core["apply_url"] = al
	else:
		jsonld_rec = extract_from_json_ld_blogs(objs, url)
		meta_rec = extract_via_meta(html, "blogs", url)
		rule_rec = apply_schema_rules(html, url, "blogs", schema)
		core = build_record(url, "blogs", schema, jsonld_rec, meta_rec, rule_rec)
	missing = validate_required(core, scrape_type)
	# Relax validation: always emit the record even if some required fields are missing
	return core, None


def read_lines(path: Path) -> List[str]:
	if not path.exists():
		return []
	try:
		return [line.strip() for line in path.read_text(encoding="utf-8").splitlines() if line.strip()]
	except Exception:
		return []


def append_jsonl(path: Path, obj: Dict[str, Any]) -> None:
	with path.open("a", encoding="utf-8") as f:
		f.write(json.dumps(obj, ensure_ascii=False) + "\n")


def append_line(path: Path, line: str) -> None:
	with path.open("a", encoding="utf-8") as f:
		f.write(line + "\n")


def slugify_text(text: str) -> str:
	text = normalize_whitespace(text.lower())
	text = re.sub(r"[^a-z0-9\s-]", "", text)
	text = re.sub(r"[\s_-]+", "-", text).strip("-")
	return text or "item"


# Enhanced extraction based on universal_jobs_scraper.py patterns
JOB_WORDS = [
	"engineer", "developer", "architect", "manager", "director", "vp", "representative",
	"sales", "marketing", "product", "designer", "analyst", "consultant", "support",
	"qa", "test", "scientist", "intern", "security", "devops", "finance", "hr", "recruiter",
	"chef", "lead", "senior", "junior", "specialist", "coordinator", "assistant"
]

JOB_URL_HINTS = [
	r"/job/", r"/jobs/", r"/careers/", r"/positions/", r"/open-positions",
	r"boards\.greenhouse\.io", r"jobs\.lever\.co", r"apply\.workable\.com",
	r"careers\.smartrecruiters\.com", r"\.bamboohr\.com/careers"
]


def _detect_ats_and_extract(listing_url: str, html: str) -> List[Dict[str, Any]]:
	"""Detect and extract from common ATS platforms using their APIs."""
	try:
		from bs4 import BeautifulSoup  # type: ignore
		import requests
		soup = BeautifulSoup(html, "html.parser")

		# Check for Greenhouse
		for a in soup.select("a[href]"):
			href = a.get("href", "")
			match = re.search(r"boards\.greenhouse\.io/([^/]+)/jobs", href)
			if match:
				company = match.group(1)
				return _fetch_greenhouse_jobs(company)

		# Check for Lever
		for a in soup.select("a[href]"):
			href = a.get("href", "")
			match = re.search(r"(?:jobs|apply)\.lever\.co/([^/?#]+)", href)
			if match:
				company = match.group(1)
				return _fetch_lever_jobs(company)

		# Check for script tags that might reference ATS
		for script in soup.find_all("script", src=True):
			src = script.get("src", "")
			if "greenhouse" in src:
				match = re.search(r"for=([a-z0-9\-]+)", src, re.I)
				if match:
					return _fetch_greenhouse_jobs(match.group(1))

		return []
	except Exception:
		return []


def _fetch_greenhouse_jobs(company: str) -> List[Dict[str, Any]]:
	"""Fetch jobs from Greenhouse API."""
	try:
		import requests
		url = f"https://boards-api.greenhouse.io/v1/boards/{company}/jobs?content=true"
		response = requests.get(url, timeout=10)
		response.raise_for_status()
		data = response.json()

		jobs = []
		for job in data.get("jobs", []):
			title = (job.get("title") or "").strip()
			job_url = job.get("absolute_url") or ""
			location = job.get("location", {}).get("name") or ""
			department = ", ".join([d.get("name","") for d in job.get("departments", []) if d.get("name")])

			if title:
				jobs.append({
					"title": title,
					"company": company.capitalize(),
					"location": location or None,
					"description": title,  # Minimal description
					"apply_url": job_url,
					"source_url": job_url,
					"department": department or None,
					"schema_version": 1,
				})
		return jobs
	except Exception:
		return []


def _fetch_lever_jobs(company: str) -> List[Dict[str, Any]]:
	"""Fetch jobs from Lever API."""
	try:
		import requests
		url = f"https://api.lever.co/v0/postings/{company}?mode=json"
		response = requests.get(url, timeout=10)
		response.raise_for_status()
		data = response.json()

		jobs = []
		for job in data:
			title = (job.get("text") or job.get("title") or "").strip()
			job_url = job.get("hostedUrl") or job.get("applyUrl") or ""
			location = ""
			department = ""

			if isinstance(job.get("categories"), dict):
				location = job["categories"].get("location") or ""
				department = job["categories"].get("team") or ""

			if title:
				jobs.append({
					"title": title,
					"company": company.capitalize(),
					"location": location or None,
					"description": title,  # Minimal description
					"apply_url": job_url,
					"source_url": job_url,
					"department": department or None,
					"schema_version": 1,
				})
		return jobs
	except Exception:
		return []


def _extract_jobs_html_fallback(listing_url: str, html: str) -> List[Dict[str, Any]]:
	"""Enhanced HTML fallback extraction based on universal scraper patterns."""
	try:
		# First try ATS detection
		ats_jobs = _detect_ats_and_extract(listing_url, html)
		if ats_jobs:
			return ats_jobs

		# Fallback to HTML scraping
		from bs4 import BeautifulSoup  # type: ignore
		soup = BeautifulSoup(html, "html.parser")
		jobs = []

		# Look for links with job-related text or URLs
		for a in soup.select("a[href]"):
			text = (a.get_text() or "").strip()
			href = a.get("href") or ""
			if not text or not href:
				continue

			# Check for job-related words in text
			jobish_text = any(word in text.lower() for word in JOB_WORDS)

			# Check for job-related URL patterns
			jobish_url = any(re.search(pattern, href, re.I) for pattern in JOB_URL_HINTS)

			# Also check for job title patterns (avoid common nav links)
			title_like = (len(text) > 10 and
						 not any(nav_word in text.lower() for nav_word in
								["home", "about", "contact", "blog", "news", "services"]))

			if (jobish_text or jobish_url or (title_like and len(text) < 100)):
				# Extract additional context from parent elements
				description = ""
				location = ""
				apply_url = None

				# Look for description in nearby elements
				parent = a.parent
				if parent:
					# Check siblings for description text
					for sibling in parent.find_all(["p", "div", "span"], limit=3):
						desc_text = sibling.get_text(" ").strip()
						if desc_text and len(desc_text) > 20:
							description = normalize_whitespace(desc_text)[:500]  # Limit length
							break

				# Look for location indicators
				context_text = ""
				if parent:
					context_text = parent.get_text(" ")

				loc_match = re.search(r"\b(Remote|Toronto|Ottawa|Vancouver|Canada|United Kingdom|UK|US|USA|MA|ON|San Francisco|New York|Austin|Seattle|Denver|Boston)\b",
									  context_text, re.IGNORECASE)
				if loc_match:
					location = loc_match.group(0)

				# Check if this is an apply link or job detail link
				if "apply" in text.lower() or "apply" in href.lower():
					apply_url = canonicalize_url(listing_url, href)

				# Create job record
				job_url = canonicalize_url(listing_url, href) or listing_url

				job_record = {
					"title": text,
					"company": _extract_host(listing_url).split(".")[-2].capitalize() if _extract_host(listing_url) else None,
					"location": location or None,
					"description": description or text,  # Use title as fallback description
					"apply_url": apply_url or job_url,
					"source_url": job_url,
					"schema_version": 1,
				}

				# Enhanced filtering for better job detection
				skip_words = [
					"home", "about", "contact", "login", "register", "careers", "view", "skip",
					"privacy", "policy", "terms", "service", "notice", "core", "values",
					"apply now", "apply", "search", "what we do", "results", "insights",
					"read more", "learn more", "get started", "sign up", "subscribe"
				]

				# Filter out obvious non-jobs
				is_valid_job = (
					len(text) >= 5 and
					len(text) <= 100 and  # Reasonable job title length
					not any(skip in text.lower() for skip in skip_words) and
					not text.lower().startswith(("http", "www", "mailto")) and
					not text.lower().endswith(("@", ".com", ".net", ".org")) and
					# Must have at least one job-related word OR be in a job-related URL
					(jobish_text or jobish_url)
				)

				if is_valid_job:
					jobs.append({k: v for k, v in job_record.items() if v})

		# Deduplicate by title and URL
		seen = set()
		unique_jobs = []
		for job in jobs:
			key = (job.get("title", ""), job.get("source_url", ""))
			if key not in seen and key[0]:  # Ensure title exists
				unique_jobs.append(job)
				seen.add(key)

		return unique_jobs[:10]  # Limit to top 10 results

	except Exception:
		return []


def extract_inline_jobs_from_listing(listing_url: str, settings: CrawlerSettings) -> List[Dict[str, Any]]:
	"""Best-effort inline job extraction for listings that embed postings (no item URLs)."""
	results: List[Dict[str, Any]] = []
	html = fetch_html(listing_url, settings)
	if not html:
		return results

	# Try enhanced HTML fallback first
	enhanced_results = _extract_jobs_html_fallback(listing_url, html)
	if enhanced_results:
		return enhanced_results

	# Fallback to original heading-based extraction
	try:
		from bs4 import BeautifulSoup  # type: ignore
		soup = BeautifulSoup(html, "html.parser")
		main = soup.find("main") or soup
		headings = main.find_all(["h2", "h3"]) if main else []
		for h in headings:
			title = normalize_whitespace(h.get_text(" "))
			if not title or len(title) < 5:
				continue
			contents: List[str] = []
			apply_url: Optional[str] = None
			location: Optional[str] = None
			for sib in h.next_siblings:
				if getattr(sib, "name", None) in ("h2", "h3"):
					break
				text = ""
				if hasattr(sib, "get_text"):
					text = sib.get_text(" ")
				else:
					text = str(sib)
				text = normalize_whitespace(text)
				if text:
					contents.append(text)
				try:
					for a in getattr(sib, "find_all", lambda *args, **kwargs: [])("a"):
						txt = (a.get_text(" ") or "").strip().lower()
						href = a.get("href") or ""
						if "apply" in txt or "apply" in href.lower():
							absu = canonicalize_url(listing_url, href)
							if absu:
								apply_url = absu
				except Exception:
					pass
			joined = " \n ".join(contents)
			loc_match = re.search(r"\b(Remote|Toronto|Ottawa|Vancouver|Canada|United Kingdom|UK|US|USA|MA|ON)\b", joined, re.IGNORECASE)
			if loc_match:
				location = loc_match.group(0)
			description = normalize_whitespace(strip_html_tags(joined))
			# Relaxed description length requirement - was 120, now 20
			if description and len(description) < 20:
				continue
			rec: Dict[str, Any] = {
				"title": title,
				"company": _extract_host(listing_url).split(".")[-2].capitalize() if _extract_host(listing_url) else None,
				"location": location,
				"description": description,
				"apply_url": apply_url or listing_url,
				"source_url": f"{listing_url}#job-{slugify_text(title)}",
				"schema_version": 1,
			}
			results.append({k: v for k, v in rec.items() if v})
		return results
	except Exception:
		return results


def ensure_extraction_for_type(
	base_dir: Path,
	scrape_type: str,
	listing_url: str,
	settings: CrawlerSettings,
	max_items: int = 10,
) -> Dict[str, Any]:
	"""Extract items for a type using discovered URLs and schema. Returns stats."""
	state_dir = base_dir / "state"
	discovered_path = state_dir / f"discovered_{scrape_type}_urls.txt"

	urls = read_lines(discovered_path)

	count_extracted = 0
	count_errors = 0
	count_skipped = 0
	extracted_data = []
	
	for url in urls:
		# Ensure a schema for the specific item domain (e.g., ATS domain)
		current_schema = ensure_schema(base_dir, scrape_type, url)
		rec, fail = extract_single_item(url, scrape_type, current_schema, settings)
		if rec:
			extracted_data.append(rec)
			count_extracted += 1
		else:
			# Try schema repair if missing required fields and we have HTML
			if fail and _llm_available():
				repaired = attempt_schema_repair_with_llm(
					current_schema,
					failed_fields=fail.get("missing", []),
					html_sample=fail.get("html", "")[:8000],
					page_url=url,
				)
				if repaired and isinstance(repaired, dict):
					# Merge and bump version for this domain's schema
					merged = dict(current_schema)
					merged["extraction_rules"] = repaired.get("extraction_rules") or merged.get("extraction_rules") or {}
					merged = bump_schema_version(merged)
					save_schema(base_dir, merged)
					# Retry once
					rec2, _ = extract_single_item(url, scrape_type, merged, settings)
					if rec2:
						extracted_data.append(rec2)
						count_extracted += 1
						continue
			count_errors += 1
		# Stop if over limit
		if count_extracted >= max_items:
			break

	return {
		"extracted": count_extracted, 
		"errors": count_errors, 
		"skipped": count_skipped,
		"data": extracted_data
	}


# ===== Output printing (separate sections) =====


def _read_jsonl_lines(path: Path) -> List[str]:
	if not path.exists():
		return []
	try:
		return [line.rstrip("\n") for line in path.read_text(encoding="utf-8").splitlines() if line.strip()]
	except Exception:
		return []


def print_jsonl_section(title: str, path: Path) -> None:
	lines = _read_jsonl_lines(path)
	print(f"=== {title} ({len(lines)}) ===")
	for line in lines:
		print(line)


# This module is now primarily used as a library by main.py
